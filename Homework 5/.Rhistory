K = 10
C = 10^seq(10, -2, length=100)
folds = sample(1:K, nrow(X), replace = TRUE)
cv.errs = rep(NA, K)
for(k in 1:K)
{
svm.model = svm(V1.1 ~ ., data = train[folds!=k,], kernel = "radial", type = "C", cost = 10^10)
svm.pred = predict(svm.model, newdata = train[folds==k,])
cv.errs[k] = sum(svm.pred != train[folds==k,32])/length(train[folds==k,32])
}
# best C is 10^10
svm.min = c(mean(cv.errs), which.min(cv.errs))
svm.min # approx. 0.1588095
# radial kernel
K = 10
C = 10^seq(10, -2, length=100)
folds = sample(1:K, nrow(X), replace = TRUE)
cv.errs = rep(NA, K)
for(k in 1:K)
{
svm.model = svm(V1.1 ~ ., data = train[folds!=k,], kernel = "radial", type = "C", cost = 10^10)
svm.pred = predict(svm.model, newdata = train[folds==k,])
cv.errs[k] = sum(svm.pred != train[folds==k,32])/length(train[folds==k,32])
}
# best C is 10^10
svm.min = c(mean(cv.errs), which.min(cv.errs))
svm.min # approx. 0.1588095
# radial kernel
K = 10
C = 10^seq(10, -2, length=100)
folds = sample(1:K, nrow(X), replace = TRUE)
cv.errs = rep(NA, K)
for(k in 1:K)
{
svm.model = svm(V1.1 ~ ., data = train[folds!=k,], kernel = "radial", type = "C", cost = 10^10)
svm.pred = predict(svm.model, newdata = train[folds==k,])
cv.errs[k] = sum(svm.pred != train[folds==k,32])/length(train[folds==k,32])
}
# best C is 10^10
svm.min = c(mean(cv.errs), which.min(cv.errs))
svm.min # approx. 0.1588095
# radial kernel
K = 10
C = 10^seq(10, -2, length=100)
folds = sample(1:K, nrow(X), replace = TRUE)
cv.errs = rep(NA, K)
for(k in 1:K)
{
svm.model = svm(V1.1 ~ ., data = train[folds!=k,], kernel = "radial", type = "C", cost = 10^10)
svm.pred = predict(svm.model, newdata = train[folds==k,])
cv.errs[k] = sum(svm.pred != train[folds==k,32])/length(train[folds==k,32])
}
# best C is 10^10
svm.min = c(mean(cv.errs), which.min(cv.errs))
svm.min # approx. 0.1588095
# radial kernel
K = 10
C = 10^seq(10, -2, length=100)
folds = sample(1:K, nrow(X), replace = TRUE)
cv.errs = rep(NA, K)
for(k in 1:K)
{
svm.model = svm(V1.1 ~ ., data = train[folds!=k,], kernel = "radial", type = "C", cost = 10^10)
svm.pred = predict(svm.model, newdata = train[folds==k,])
cv.errs[k] = sum(svm.pred != train[folds==k,32])/length(train[folds==k,32])
}
# best C is 10^10
svm.min = c(mean(cv.errs), which.min(cv.errs))
svm.min # approx. 0.1588095
# radial kernel
K = 10
C = 10^seq(10, -2, length=100)
folds = sample(1:K, nrow(X), replace = TRUE)
cv.errs = rep(NA, K)
for(k in 1:K)
{
svm.model = svm(V1.1 ~ ., data = train[folds!=k,], kernel = "radial", type = "C", cost = 10^10)
svm.pred = predict(svm.model, newdata = train[folds==k,])
cv.errs[k] = sum(svm.pred != train[folds==k,32])/length(train[folds==k,32])
}
# best C is 10^10
svm.min = c(mean(cv.errs), which.min(cv.errs))
svm.min # approx. 0.1588095
#### SVM ####
# radial kernel
K = nrow(X)
folds = 1:K
cv.errs = rep(NA, K)
for(k in 1:K)
{
svm.model = svm(V1.1 ~ ., data = train[folds!=k,], kernel = "radial", type = "C", cost = 10^10)
svm.pred = predict(svm.model, newdata = train[folds==k,])
cv.errs[k] = sum(svm.pred != train[folds==k,32])/length(train[folds==k,32])
}
# best C is 10^10
svm.min = c(mean(cv.errs), which.min(cv.errs))
svm.min # approx. 0.1588095
#### SVM ####
# radial kernel
K = nrow(X)
folds = 1:K
cv.errs = rep(NA, K)
for(k in 1:K)
{
svm.model = svm(V1.1 ~ ., data = train[folds!=k,], kernel = "radial", type = "C", cost = 10^10)
svm.pred = predict(svm.model, newdata = train[folds==k,])
cv.errs[k] = sum(svm.pred != train[folds==k,32])/length(train[folds==k,32])
}
# best C is 10^10
svm.min = c(mean(cv.errs), which.min(cv.errs))
svm.min # approx. 0.1588095
#### SVM ####
# radial kernel
K = nrow(X)
folds = 1:K
cv.errs = rep(NA, K)
for(k in 1:K)
{
svm.model = svm(V1.1 ~ ., data = train[folds!=k,], kernel = "radial", type = "C", cost = 10^10)
svm.pred = predict(svm.model, newdata = train[folds==k,])
cv.errs[k] = sum(svm.pred != train[folds==k,32])/length(train[folds==k,32])
}
# best C is 10^10
svm.min = c(mean(cv.errs), which.min(cv.errs))
svm.min # approx. 0.1588095
K = nrow(X)
folds = 1:K
cv.errs = rep(NA, K)
for(k in 1:K)
{
svm.model = svm(V1.1 ~ ., data = train[folds!=k,], kernel = "polynomial", degree = 3, type = "C", cost = 10^10)
svm.pred = predict(svm.model, newdata = train[folds==k,])
cv.errs[k] = sum(svm.pred != train[folds==k,32])/length(train[folds==k,32])
}
# best C is 10^10
svm.min = c(mean(cv.errs), which.min(cv.errs))
svm.min # approx. 0.195122
K = nrow(X)
folds = 1:K
cv.errs = rep(NA, K)
for(k in 1:K)
{
svm.model = svm(V1.1 ~ ., data = train[folds!=k,], kernel = "polynomial", degree = 3, type = "C", cost = 10^10)
svm.pred = predict(svm.model, newdata = train[folds==k,])
cv.errs[k] = sum(svm.pred != train[folds==k,32])/length(train[folds==k,32])
}
# best C is 10^10
svm.min = c(mean(cv.errs), which.min(cv.errs))
svm.min # approx. 0.195122
K = nrow(X)
folds = 1:K
cv.errs = rep(NA, K)
for(k in 1:K)
{
svm.model = svm(V1.1 ~ ., data = train[folds!=k,], kernel = "polynomial", degree = 2, type = "C", cost = 10^10)
svm.pred = predict(svm.model, newdata = train[folds==k,])
cv.errs[k] = sum(svm.pred != train[folds==k,32])/length(train[folds==k,32])
}
# best C is 10^10
svm.min = c(mean(cv.errs), which.min(cv.errs))
svm.min # approx. 0.2560976
K = nrow(X)
folds = 1:K
cv.errs = rep(NA, K)
for(k in 1:K)
{
svm.model = svm(V1.1 ~ ., data = train[folds!=k,], kernel = "polynomial", degree = 1, type = "C", cost = 10^10)
svm.pred = predict(svm.model, newdata = train[folds==k,])
cv.errs[k] = sum(svm.pred != train[folds==k,32])/length(train[folds==k,32])
}
# best C is 10^10
svm.min = c(mean(cv.errs), which.min(cv.errs))
svm.min # approx. 0.2560976
K = nrow(X)
folds = 1:K
cv.errs = rep(NA, K)
for(k in 1:K)
{
svm.model = svm(V1.1 ~ ., data = train[folds!=k,], kernel = "polynomial", degree = 4, type = "C", cost = 10^10)
svm.pred = predict(svm.model, newdata = train[folds==k,])
cv.errs[k] = sum(svm.pred != train[folds==k,32])/length(train[folds==k,32])
}
# best C is 10^10
svm.min = c(mean(cv.errs), which.min(cv.errs))
svm.min # approx. 0.2560976
K = nrow(X)
folds = 1:K
cv.errs = rep(NA, K)
for(k in 1:K)
{
svm.model = svm(V1.1 ~ ., data = train[folds!=k,], kernel = "polynomial", degree = 5, type = "C", cost = 10^10)
svm.pred = predict(svm.model, newdata = train[folds==k,])
cv.errs[k] = sum(svm.pred != train[folds==k,32])/length(train[folds==k,32])
}
# best C is 10^10
svm.min = c(mean(cv.errs), which.min(cv.errs))
svm.min # approx. 0.2560976
K = nrow(X)
folds = 1:K
cv.errs = rep(NA, K)
for(k in 1:K)
{
svm.model = svm(V1.1 ~ ., data = train[folds!=k,], kernel = "polynomial", degree = 1, type = "C", cost = 10^10)
svm.pred = predict(svm.model, newdata = train[folds==k,])
cv.errs[k] = sum(svm.pred != train[folds==k,32])/length(train[folds==k,32])
}
# best C is 10^10
svm.min = c(mean(cv.errs), which.min(cv.errs))
svm.min # approx. 0.2560976
K = nrow(X)
folds = 1:K
cv.errs = rep(NA, K)
for(k in 1:K)
{
svm.model = svm(V1.1 ~ ., data = train[folds!=k,], kernel = "polynomial", degree = 1, type = "C", cost = 9^10)
svm.pred = predict(svm.model, newdata = train[folds==k,])
cv.errs[k] = sum(svm.pred != train[folds==k,32])/length(train[folds==k,32])
}
# best C is 10^10
svm.min = c(mean(cv.errs), which.min(cv.errs))
svm.min # approx. 0.2195122
svm.pred = predict(svm.model, newdata = Xn)
svm.pred
svm.model = svm(V1.1 ~ ., data = train, kernel = "radial", type = "C", cost = 10^10)
svm.pred.radial = predict(svm.model, newdata = Xn)
svm.pred.radial
svm.radial = svm(V1.1 ~ ., data = train, kernel = "radial", type = "C", cost = 10^10)
svm.pred.radial = predict(svm.radial, newdata = Xn)
svm.pred.radial
svm.poly = svm(V1.1 ~ ., data = train, kernel = "polynomial", degree = 1, type = "C", cost = 10^10)
svm.pred.poly = predict(svm.poly, newdata = Xn)
svm.pred.poly
dim(svm.pred.poly)
svm.pred
rf.model = randomForest(as.factor(V1.1)~., data = train, importance = TRUE, mtry = 1)
rf.pred = predict(rf.model, newdata = Xn, type="response")
rf.pred
lda.pred = predict(lda.mod, newdata = Xn)
lda.mod = lda(V1.1~., data = train)
lda.pred = predict(lda.mod, newdata = Xn)
lda.pred
lda.pred$class
length(lda.pred$class)
glm.model.r = glmnet(Xglm, Yglm, alpha=0, lambda=lam, family = "binomial")
glm.pred.r = predict(glm.model.r, s=lam[l], newx=Xn, type = "class")  # ridge
glm.pred.r = predict(glm.model.r, s=lam[l], newx=as.matrix(Xn), type = "class")  # ridge
gbm.mod = gbm(V1.1 ~., data=train, distribution = "bernoulli", n.trees = 2000, interaction.depth = 1, shrinkage = 0.01, n.minobsinnode = 10)
gbm.predtest = predict(gbm.mod, newdata = Xn)
gbm.predtest = predict(gbm.mod, newdata = Xn, ntrees = 2000)
gbm.predtest = predict(gbm.mod, newdata = Xn, n.trees = 2000)
length(gbm.predtest)
gbm.predtest
gbm.predtest = predict(gbm.mod, newdata = Xn, n.trees = 2000, type="response")
length(gbm.predtest)
gbm.mod = gbm(V1.1 ~., data=train, distribution = "bernoulli", n.trees = 2000, interaction.depth = 1, shrinkage = 0.01, n.minobsinnode = 10)
gbm.predtest = predict(gbm.mod, newdata = Xn, n.trees = 2000, type="response")
length(gbm.predtest)
gbm.predtest
gbm.final = round(gbm.predtest)
gbm.final
km.model = kmeans(X, 2, nstart = 50)
c = km.model$cluster - 1
c
#### K-MEANS ####
km.model = kmeans(Xn, 2, nstart = 50)
c = km.model$cluster - 1
c
km.model = kmeans(Xn, 2, nstart = 50)
km.cluster = km.model$cluster - 1
km.cluster
compare = rbind(svm.pred.poly, svm.pred.radial, gbm.final, rf.pred, lda.pred$class, km.cluster)
compare
svm.pred.poly
compare
rf.pred
as.list(rf.pred)
c(rf.pred)
as.vector(rf.pred)
compare = rbind(svm.pred.poly, svm.pred.radial, gbm.final, as.vector(rf.pred), lda.pred$class, km.cluster)
compare
svm.pred.poly
svm.pred.radial
gbm.final
rf.pred
lda.pred$class
km.cluster
typeof(km.cluster)
compare = rbind(as.double(svm.pred.poly), as.double(svm.pred.radial), as.double(gbm.final), as.double(rf.pred), lda.pred$class, km.cluster)
compare
names(compare) <- c("SVM POLY", "SVM RADIAL", "GBM", "RF", "LDA", "K-MEANS")
compare
?names
names(compare) <- r("SVM POLY", "SVM RADIAL", "GBM", "RF", "LDA", "K-MEANS")
names(compare) <- rbind("SVM POLY", "SVM RADIAL", "GBM", "RF", "LDA", "K-MEANS")
compare
compare = rbind(as.double(svm.pred.poly), as.double(svm.pred.radial), as.double(gbm.final), as.double(rf.pred), lda.pred$class, km.cluster)
compare
View(compare)
View(compare)
as.double(svm.pred.poly)
as.double(svm.pred.radial)
as.double(gbm.final)
as.double(rf.pred)
lda.pred$class
km.cluster
as.double(lda.pred$class)
as.double(lda.pred$class - 1)
as.double(lda.pred$class) - 1
as.double(svm.pred.poly)
as.double(svm.pred.radial)
as.double(gbm.final)
as.double(rf.pred) - 1
as.double(lda.pred$class) - 1
km.cluster
as.double(svm.pred.poly) - 1
as.double(svm.pred.radial) - 1
as.double(gbm.final)
as.double(rf.pred) - 1
as.double(lda.pred$class) - 1
km.cluster
SVMPOLY1 = as.double(svm.pred.poly) - 1
SVMRADIAL = as.double(svm.pred.radial) - 1
GBM1 = as.double(gbm.final)
RF1 = as.double(rf.pred) - 1
LDA = as.double(lda.pred$class) - 1
KMC = km.cluster
compare = rbind(SVMPOLY1, SVMRADIAL, GBM1, RF1, LDA, KMC)
compare
svm.min2 = c(mean(cv.errs), which.min(cv.errs))
svm.min2 # approx. 0.2195122
compare = cbind(compare, c(svm.min2, svm.min, 0.15, rf.min, lda.min, 0.1))
compare
svm.min = c(mean(cv.errs), which.min(cv.errs))
svm.min # approx. 0.195122
# radial kernel
K = nrow(X)
folds = 1:K
cv.errs = rep(NA, K)
for(k in 1:K)
{
svm.model = svm(V1.1 ~ ., data = train[folds!=k,], kernel = "radial", type = "C", cost = 10^10)
svm.pred = predict(svm.model, newdata = train[folds==k,])
cv.errs[k] = sum(svm.pred != train[folds==k,32])/length(train[folds==k,32])
}
# best C is 10^10
svm.min = c(mean(cv.errs), which.min(cv.errs))
svm.min # approx. 0.195122
set.seed(3)
K = nrow(X)
folds = 1:K
rf.errors = rep(NA, K)
# try = tuneRF(x = train[,1:31], y = train[,32], mtryStart = 1, ntreeTry = 1000)
for(k in 1:K)
{
rf.model = randomForest(as.factor(V1.1)~., data = train[folds!=k,], importance = TRUE, mtry = 1)
rf.pred = predict(rf.model, newdata = train[folds==k,1:31], type="response")
rf.errors[k] = sum(rf.pred != train[folds==k,32])/length(train[folds==k,32])
}
rf.min = mean(rf.errors)
rf.min # approx. 0.1653117
compare = cbind(compare, c(svm.min2, svm.min, 0.15, rf.min, lda.min, 0.1))
compare
compare = rbind(SVMPOLY1, SVMRADIAL, GBM1, RF1, LDA, KMC)
compare
compare = cbind(compare, c(svm.min2, svm.min, 0.15, rf.min, lda.min, 0.1))
compare
# radial kernel
K = nrow(X)
folds = 1:K
cv.errs = rep(NA, K)
for(k in 1:K)
{
svm.model = svm(V1.1 ~ ., data = train[folds!=k,], kernel = "radial", type = "C", cost = 10^10)
svm.pred = predict(svm.model, newdata = train[folds==k,])
cv.errs[k] = sum(svm.pred != train[folds==k,32])/length(train[folds==k,32])
}
# best C is 10^10
svm.min = c(mean(cv.errs), which.min(cv.errs))
svm.min # approx. 0.195122
rf.min # approx. 0.1653117
compare = cbind(compare, c(svm.min2, svm.min, 0.15, rf.min, lda.min, 0.1))
compare = rbind(SVMPOLY1, SVMRADIAL, GBM1, RF1, LDA, KMC)
compare = cbind(compare, c(svm.min2, svm.min, 0.15, rf.min, lda.min, 0.1))
compare
compare[2,52] = sv.min
compare[2,52] = svm.min
svm.min
compare = rbind(SVMPOLY1, SVMRADIAL, GBM1, RF1, LDA, KMC)
compare = cbind(compare, c(svm.min2, svm.min[1], 0.15, rf.min[1], lda.min, 0.1))
comapre
compare
svm.min # approx. 0.195122
compare = rbind(SVMPOLY1, SVMRADIAL, GBM1, RF1, LDA, KMC)
compare = cbind(compare, c(svm.min2, svm.min[1], 0.15, rf.min[1], lda.min, 0.1))
svm.min[1]
compare
compare = rbind(SVMPOLY1, SVMRADIAL, GBM1, RF1, LDA, KMC)
compare = cbind(compare, c(svm.min2, 0.195122, 0.15, rf.min[1], lda.min, 0.1))
compare
compare = rbind(SVMPOLY1, SVMRADIAL, GBM1, RF1, LDA, KMC)
compare
compare[2,11]
compare[2,12]
svmp = 0.2195122
svmr = 0.195122
gbm1 = 0.15
rf1 = 0.1653117
lda = 0.2987879
kmc = 0.15
c(svmp, svmr, gbm1, rf1, lda, kmc)
compare = cbind(compare, c(svmp, svmr, gbm1, rf1, lda, kmc))
compare
compare = rbind(SVMPOLY1, SVMRADIAL, GBM1, RF1, LDA, KMC)
compare = cbind(compare, 1 - c(svmp, svmr, gbm1, rf1, lda, kmc))
compare
total = sum(1 - c(svmp, svmr, gbm1, rf1, lda, kmc))
total
compare = rbind(SVMPOLY1, SVMRADIAL, GBM1, RF1, LDA, KMC)
compare
svmp = 0.2195122
svmr = 0.195122
gbm1 = 0.15
rf1 = 0.1653117
lda = 0.2987879
kmc = 0.15
weights = 1 - c(svmp, svmr, gbm1, rf1, lda, kmc)
weights
svmp = 0.2195122
svmr = 0.195122
gbm1 = 0.15
rf1 = 0.1653117
lda = 0.2987879
kmc = 0.15
weights = 1 - c(svmp, svmr, gbm1, rf1, lda, kmc)
SVMPOLY1 = (as.double(svm.pred.poly) - 1) * svmp
SVMRADIAL = (as.double(svm.pred.radial) - 1) * svmr
GBM1 = as.double(gbm.final) * gbm1
RF1 = (as.double(rf.pred) - 1) * rf1
LDA = (as.double(lda.pred$class) - 1) * lda
KMC = km.cluster * kmc
compare = rbind(SVMPOLY1, SVMRADIAL, GBM1, RF1, LDA, KMC)
compare
svmp = 1 - 0.2195122
svmr = 1 - 0.195122
gbm1 = 1 - 0.15
rf1 = 1 - 0.1653117
lda = 1 - 0.2987879
kmc = 1 - 0.15
weights = 1 - c(svmp, svmr, gbm1, rf1, lda, kmc)
SVMPOLY1 = (as.double(svm.pred.poly) - 1) * svmp
SVMRADIAL = (as.double(svm.pred.radial) - 1) * svmr
GBM1 = as.double(gbm.final) * gbm1
RF1 = (as.double(rf.pred) - 1) * rf1
LDA = (as.double(lda.pred$class) - 1) * lda
KMC = km.cluster * kmc
compare = rbind(SVMPOLY1, SVMRADIAL, GBM1, RF1, LDA, KMC)
compare
svmp = 1 - 0.2195122
svmr = 1 - 0.195122
gbm1 = 1 - 0.15
rf1 = 1 - 0.1653117
lda = 1 - 0.2987879
kmc = 1 - 0.15
weights = c(svmp, svmr, gbm1, rf1, lda, kmc)
total = sum(weights)
SVMPOLY1 = (as.double(svm.pred.poly) - 1) * svmp / total
SVMRADIAL = (as.double(svm.pred.radial) - 1) * svmr / total
GBM1 = as.double(gbm.final) * gbm1 / total
RF1 = (as.double(rf.pred) - 1) * rf1 / total
LDA = (as.double(lda.pred$class) - 1) * lda / total
KMC = km.cluster * kmc / total
compare = rbind(SVMPOLY1, SVMRADIAL, GBM1, RF1, LDA, KMC)
compare
sum(compare[,1])
rf1
RF1
GBM1
as.double(rf.pred)
rf.pred
rf.pred ###########################
rf.model = randomForest(as.factor(V1.1)~., data = train, importance = TRUE, mtry = 1)
rf.pred = predict(rf.model, newdata = Xn, type="response")
rf.pred ###########################
svmp = 1 - 0.2195122
svmr = 1 - 0.195122
gbm1 = 1 - 0.15
rf1 = 1 - 0.1653117
lda = 1 - 0.2987879
kmc = 1 - 0.15
weights = c(svmp, svmr, gbm1, rf1, lda, kmc)
total = sum(weights)
SVMPOLY1 = (as.double(svm.pred.poly) - 1) * svmp / total
SVMRADIAL = (as.double(svm.pred.radial) - 1) * svmr / total
GBM1 = as.double(gbm.final) * gbm1 / total
RF1 = (as.double(rf.pred) - 1) * rf1 / total
LDA = (as.double(lda.pred$class) - 1) * lda / total
KMC = km.cluster * kmc / total
compare = rbind(SVMPOLY1, SVMRADIAL, GBM1, RF1, LDA, KMC)
compare
sum(compare[,1])
vote = apply(compare, 2, mean)
vote
vote = apply(compare, 2, sum)
vote
vote = round(apply(compare, 2, sum))
vote
(as.double(svm.pred.poly) - 1)
(as.double(svm.pred.radial) - 1)
as.double(gbm.final)
(as.double(rf.pred) - 1)
(as.double(lda.pred$class) - 1)
km.cluster
