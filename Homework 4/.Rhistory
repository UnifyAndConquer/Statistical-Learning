setwd("~/Desktop/AUB/2017-2018/2017-2018Spring/STAT 239/Statistical-Learning/Homework 4")
# K-means clustering
set.seed(42)
X = read.table("test.txt", header = T)
X
train = read.table("test.txt", header = T)
X = model.matrix(y~., data = train)
pcr.model = prcomp(X)
pcr.model
summary(pcr.model)
pcr.model = prcomp(X, retx = TRUE)
summary(pcr.model)
pcr.model = prcomp(X, retx = TRUE, center = TRUE, scale = TRUE)
pcr.model = prcomp(X, retx = TRUE)
pcr.model$x
PC1 = pcr.model$x[,PC1]
PC1 = pcr.model$x[,"PC1"]
PC1
PC2 = pcr.model$x[,"PC2"]
plot(PC1, PC2)
install.packages("scatterplot3d")
library(scatterplot3d)
PC3 = pcr.model$x[,"PC3"]
surf3D(PC1, PC2, PC3)
plot3d(PC1, PC2, PC3)
scatterplot3d(PC1, PC2, PC3)
persp(PC1, PC2, PC3)
scatterplot3d
km.model = kmeans(X, 2, nstart = 50)
km.model$cluster
c = km.model$cluster
ones = c["1"]
ones
c = km.model$cluster - 1
which(c)
c
which(1)
colors = replace(c, c(1,2,3,4,5,6), values = c("red", "blue"))
scatterplot3d(PC1, PC2, PC3, color = colors)
scatterplot3d(PC1, PC2, PC3, color = c("red", "blue"))
scatterplot3d(PC1, PC2, PC3, color = c("red", "blue"))
scatterplot3d(PC1, PC2, PC3, color = c("red"))
scatterplot3d(PC1, PC2, PC3, color = c("blue"))
c = km.model$cluster
c
c = km.model$cluster - 1
colors = ifelse(c, "red", "blue")
colors
scatterplot3d(PC1, PC2, PC3, color = colors)
scatterplot3d(PC1, PC2, PC3, color = colors, main = "k-means clustering with two classes")
plot(PC1, PC2, col = colors, main = "k-means clustering with two classes")
plot(PC1, PC2, col = colors, main = "k-means clustering with two classes", ylab = "PC2")
plot(PC1, PC2, col = colors, main = "k-means clustering with two classes", xlab = "PC2")
plot(PC1, PC2, col = colors, main = "k-means clustering with two classes", ylab = "PC2")
plot(x=PC1, y=PC2, col = colors, main = "k-means clustering with two classes", ylab = "PC2")
plot(x=PC1, y=PC2,  ylab = "PC2", col = colors, main = "k-means clustering with two classes")
plot(x=PC1, y=PC2,  ylab = "PC2", col = colors, main = "k-means clustering with two classes")
Y = train$y
Y
Y = train$y - 1
c
Y
# let's find out if the clusters correspond to the y values
km.err = sum(Y != c)
km.err
scatterplot3d
scatterplot3d(PC1, PC2, PC3, color = colors, main = "k-means clustering with two classes")
scatterplot3d(PC1, PC2, PC3, color = colors, main = "k-means clustering with two classes", type="h")
scatterplot3d(PC1, PC2, PC3, color = colors, main = "k-means clustering with two classes", type="l")
scatterplot3d(PC1, PC2, PC3, color = colors, main = "k-means clustering with two classes", type="h")
scatterplot3d(PC2, PC1, PC3, color = colors, main = "k-means clustering with two classes", type="h")
scatterplot3d(PC1, PC2, PC3, color = colors, main = "k-means clustering with two classes", type="h")
scatterplot3d(PC1, PC3, PC2, color = colors, main = "k-means clustering with two classes", type="h")
2
scatterplot3d(PC1, PC2, PC3, color = colors, main = "k-means clustering with two classes", type="h")
scatterplot3d(PC1, PC2, PC3, color = colors, main = "k-means clustering with two classes", type="h
p")
scatterplot3d(PC1, PC2, PC3, color = colors, main = "k-means clustering with two classes", type="p")
scatterplot3d(PC1, PC2, PC3, color = colors, main = "k-means clustering with two classes", type="h")
scatterplot3d(PC1, PC2, PC3, color = colors, main = "k-means clustering with two classes", type="h", angle = 60)
scatterplot3d(PC1, PC2, PC3, color = colors, main = "k-means clustering with two classes", type="h", angle = 70)
scatterplot3d(PC1, PC2, PC3, color = colors, main = "k-means clustering with two classes", type="h", angle = 80)
scatterplot3d(PC1, PC2, PC3, color = colors, main = "k-means clustering with two classes", type="h", angle = 60)
scatterplot3d(PC1, PC2, PC3, color = colors, main = "k-means clustering with two classes", type="h", angle = 70)
#### HIERARCHICAL CLUSTERING ####
hc.model = hclust(X, method="complete")
#### HIERARCHICAL CLUSTERING ####
hc.model = hclust(dist(X), method="complete")
hc.model
hc.model$labels
hc.model$cluster
as.dendrogram((hc.model))
plot(as.dendrogram((hc.model)))
plot(as.dendrogram((hc.model)), leaflab=NULL)
plot(as.dendrogram((hc.model)), leaflab=NULL)
plot(as.dendrogram((hc.model)))
plot(hc.model)
plot(hc.model, cex=0.8)
plot(hc.model, cex=0.6)
plot(hc.model, cex=0.5)
#### HIERARCHICAL CLUSTERING ####
hc.model = hclust(dist(X), method="single")
plot(hc.model, cex=0.5)
#### HIERARCHICAL CLUSTERING ####
hc.model = hclust(dist(X), method="complete")
plot(hc.model, cex=0.5)
plot(hc.model, cex=0.5, main="cluster dendrogram with complete linkage")
plot(hc.model, cex=0.5, main="cluster dendrogram with complete linkage and euclidean distance")
plot(hc.model, cex=0.5, main="cluster dendrogram with complete linkage and euclidean distance")
rf.model = randomForest(data = X, proximity = TRUE, importance = TRUE)
library(randomForest)
rf.model = randomForest(data = X, proximity = TRUE, importance = TRUE)
rf.model = randomForest(X, proximity = TRUE, importance = TRUE)
rf.model$proximity
rf.prox = rf.model$proximity
hc.model2 = hclust(rf.prox, method="complete")
hc.model2 = hclust(as.dist(rf.prox), method="complete")
plot(hc.model2, cex=0.5, main="cluster dendrogram with complete linkage and RF distance")
plot(hc.model, cex=0.5, main="complete linkage and euclidean distance")
plot(hc.model2, cex=0.5, main="complete linkage and RF distance")
scatterplot3d(PC1, PC2, PC3, color = colors, main = "k-means clustering with two classes", type="h", angle = 70)
plot(x=PC1, y=PC2,  ylab = "PC2", col = colors, main = "k-means clustering with two classes")
plot(x=PC1, y=PC2,  ylab = "PC2", col = colors, main = "k-means clustering with two classes")
# correlation-based distance
corr(X)
# correlation-based distance
xcorr(X)
# correlation-based distance
cor(X)
hc.model2 = hclust(as.dist(cor(X)), method="complete")
nrow(col(X))
nrow(cor(X))
ncol(cor(X))
# correlation-based distance
cor = cor(X)[2:22, 2:22]
cor
nrow(cor)
ncol(cor)
hc.model2 = hclust(as.dist(cor, method="complete")
hc.model2 = hclust(as.dist(cor), method="complete")
# correlation-based distance
cor = cor(X)[2:22, 2:22]
hc.model2 = hclust(as.dist(cor), method="complete")
hc.model3 = hclust(as.dist(cor), method="complete")
plot(hc.model3, cex=0.5, main="complete linkage and RF distance")
plot(hc.model3, cex=0.5, main="complete linkage and correlation-based distance")
plot(hc.model3, cex=1, main="complete linkage and correlation-based distance")
PC1 = pcr.model$x[,"PC1"]
PC1
# 3d plot of first three principal components for visualization
pcr.model = prcomp(X, retx = TRUE)
pcr.model
plot(x=PC1, y=PC2,  ylab = "PC2", col = colors, main = "k-means clustering with two classes")
abline(v = 0)
abline(v = 0, lty=2)
plot(x=PC1, y=PC2,  ylab = "PC2", col = colors, main = "k-means clustering with two classes")
abline(v = 0, lty=2)
abline(v = -0.1, lty=2)
abline(v = -0.7, lty=2)
plot(x=PC1, y=PC2,  ylab = "PC2", col = colors, main = "k-means clustering with two classes")
abline(v = -0.7, lty=2)
library(ISLR)
library(glmnet)
library(rms)
library(pls)
library(tree)
library(randomForest)
library(gbm)
library(e1071)
set.seed(42)
train = read.table("train.txt", header = T)
test = read.table("test.txt", header = T)
x.train = model.matrix(y ~., train)
train$y = train$y - 1
y.train = train$y
x.test = model.matrix(y ~., test)
test$y = test$y - 1
y.test = test$y
lam = 10^seq(10, -2, length=100)
setwd("~/Desktop/AUB/2017-2018/2017-2018Spring/STAT 239/Statistical-Learning/Homework 4/Backup")
load("bt_err.txt")
load("rf_err.txt")
plot(1:R, rf.err, type = "l", ylab="average number of misclassifications", xlab = "number of trees", main = "CV error comparison of RF and GBM")
lines(1:R, bt.err, type = "l", col = "blue")
legend("topright", legend = c("Random Forest, mtry = 1", "GBM, M = 1 (stumps)"), col = c("black", "blue"), lty=1)
R = 2000  # number of trees to try RF and Boosting on
plot(1:R, rf.err, type = "l", ylab="average number of misclassifications", xlab = "number of trees", main = "CV error comparison of RF and GBM")
lines(1:R, bt.err, type = "l", col = "blue")
legend("topright", legend = c("Random Forest, mtry = 1", "GBM, M = 1 (stumps)"), col = c("black", "blue"), lty=1)
k = 10
folds = sample(1:k, nrow(x.train), replace = T)
cv.errors = matrix(NA, k, 100)
shrink = rep(0, length(lam))   # shrinkage factor for lasso
#### COMPUTE TEST ERROR FOR EVERY LAMDBA AT EVERY FOLD ####
for(i in 1:k)
{
lasso.model = glmnet(x.train[folds!=i,], y.train[folds!=i], alpha=1, lambda=lam)
for(l in 1:length(lam))
{
lasso.pred = predict.glmnet(lasso.model, s=lam[l], newx=x.train[folds==i,])
cv.errors[i, l] = mean((y.train[folds==i] - t(lasso.pred))^2)
# compute shrinkage factor
shrink[l] = sum(lasso.model$beta[,l] / sum(lasso.model$beta[,100]))
}
}
std <- function(x) sqrt(var(x)/length(x))
stderr = apply(cv.errors, 2, std)
mean.cv.errors = apply(cv.errors, 2, mean)
plot(shrink, mean.cv.errors, xlim=c(0,1), ylim=c(0.05, 0.3), ylab="lasso CV error (assuming orthogonal X)", xlab="shrinkage factor", main="Lasso CV error VS shrinkage", cex=2, type="l")
errbar(shrink, mean.cv.errors, yplus=mean.cv.errors+stderr, yminus=mean.cv.errors-stderr, add=TRUE)
lasso.best = min(mean.cv.errors) ##  0.08797778
bestS = shrink[which.min(mean.cv.errors)]
abline(v = bestS, lty=2)
set.seed(42)
train = read.table("train.txt", header = T)
test = read.table("test.txt", header = T)
x.train = model.matrix(y ~., train)
train$y = train$y - 1
y.train = train$y
x.test = model.matrix(y ~., test)
test$y = test$y - 1
y.test = test$y
lam = 10^seq(10, -2, length=100)
setwd("~/Desktop/AUB/2017-2018/2017-2018Spring/STAT 239/Statistical-Learning/Homework 4")
set.seed(42)
train = read.table("train.txt", header = T)
test = read.table("test.txt", header = T)
x.train = model.matrix(y ~., train)
train$y = train$y - 1
y.train = train$y
x.test = model.matrix(y ~., test)
test$y = test$y - 1
y.test = test$y
lam = 10^seq(10, -2, length=100)
for(i in 1:k)
{
lasso.model = glmnet(x.train[folds!=i,], y.train[folds!=i], alpha=1, lambda=lam)
for(l in 1:length(lam))
{
lasso.pred = predict.glmnet(lasso.model, s=lam[l], newx=x.train[folds==i,])
cv.errors[i, l] = mean((y.train[folds==i] - t(lasso.pred))^2)
# compute shrinkage factor
shrink[l] = sum(lasso.model$beta[,l] / sum(lasso.model$beta[,100]))
}
}
std <- function(x) sqrt(var(x)/length(x))
stderr = apply(cv.errors, 2, std)
mean.cv.errors = apply(cv.errors, 2, mean)
plot(shrink, mean.cv.errors, xlim=c(0,1), ylim=c(0.05, 0.3), ylab="lasso CV error (assuming orthogonal X)", xlab="shrinkage factor", main="Lasso CV error VS shrinkage", cex=2, type="l")
errbar(shrink, mean.cv.errors, yplus=mean.cv.errors+stderr, yminus=mean.cv.errors-stderr, add=TRUE)
lasso.best = min(mean.cv.errors) ##  0.08797778
bestS = shrink[which.min(mean.cv.errors)]
abline(v = bestS, lty=2)
k = 10
folds = sample(1:k, nrow(x.train), replace = T)
cv.errors = matrix(NA, k, 100)
shrink = rep(0, length(lam))   # shrinkage factor for lasso
for(i in 1:k)
{
lasso.model = glmnet(x.train[folds!=i,], y.train[folds!=i], alpha=1, lambda=lam)
for(l in 1:length(lam))
{
lasso.pred = predict.glmnet(lasso.model, s=lam[l], newx=x.train[folds==i,])
cv.errors[i, l] = mean((y.train[folds==i] - t(lasso.pred))^2)
# compute shrinkage factor
shrink[l] = sum(lasso.model$beta[,l] / sum(lasso.model$beta[,100]))
}
}
std <- function(x) sqrt(var(x)/length(x))
stderr = apply(cv.errors, 2, std)
mean.cv.errors = apply(cv.errors, 2, mean)
plot(shrink, mean.cv.errors, xlim=c(0,1), ylim=c(0.05, 0.3), ylab="lasso CV error (assuming orthogonal X)", xlab="shrinkage factor", main="Lasso CV error VS shrinkage", cex=2, type="l")
errbar(shrink, mean.cv.errors, yplus=mean.cv.errors+stderr, yminus=mean.cv.errors-stderr, add=TRUE)
lasso.best = min(mean.cv.errors) ##  0.08797778
bestS = shrink[which.min(mean.cv.errors)]
abline(v = bestS, lty=2)
varImpPlot(rf.model2)
rf.model2 = randomForest(as.factor(y) ~., data=train, importance = T, mtry = 1)
rf.pred2 = predict(rf.model2, newdata = test, type="response")
rf.testerr = sum(rf.pred2 != y.test) # number of misclassifications for random forest with m = 1 on the test set
varImpPlot(rf.model2)
partialPlot(rf.model2, pred.data=train, x.var=impvar[1])
imp <- importance(rf.model2)
impvar <- rownames(imp)[order(imp[, 1], decreasing=TRUE)]
partialPlot(rf.model2, pred.data=train, x.var=impvar[1])
varImpPlot(rf.model2)
